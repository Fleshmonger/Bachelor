\chapter{Project Process}
The project was developed with the agile development process, but no particular agile method was used. Design and implementation was iterated upon across short week-long sprints. After each there was an evaluation of the agent's current status and new goals would be identified for the next sprint. The project plan took form as a backlog, kept during development, and was used when determining the status and goals in scope of the whole time-plan. There were a few milestones throughout development, usually a month apart. They determined some requirements the agent should satisfy at specific points in the development, with regards to its performance. The milestones served as a medium between sprints and the overall goal, ensuring the bot improved consistently, and that a satisfactory end-result would be met.

Testing was done in two steps. First the robustness of the implementations was tested by playing the bot with random built-in StarCraft bots, across all maps used in the competitions. This was to ensure the stability of the bot and that the implementations worked as required, across all the maps it would play on. The second step was uploading the bot to the SCCAIT tournament page, where the bot would play against other bots. This would serve to test the strategic integrity of the bot, and was done rarer than the first step in order to retrieve enough statistical data. This was much slower as the matches were tested in real-time, but necessary to ensure the opponents were up-to-date. The opponents on the SSCAIT website should be as recent since the last tournament (January, 2015), and would therefore be the best candidates to test against the agent's abilities. Because it is time-consuming to retrieve data from rounds on the SCCAIT, the first step was needed to determine crash issues and such technicalities, in order to not waste time.

\section{Project Plan}
The project started in mid-January and development ceased early June, with four milestones in between. We will briefly explain them:

%Timeline

The first milestone were attaining the minimum viable product by the end of February. The requirements for this was an agent capable of defeating a passive opponent, implying means to gather resources, build units, scouting and attacking. This would imply the agent had a non-zero chance at winning a match.

Second milestone was generalizing and polishing the first solutions at the end of March, executing the agent's strategy more efficiently. This included efficient resource gathering and production, simple combat prediction and information manager of enemy units.

By late April, the third milestone required more strategic elements such as base-defense, better unit grouping with retreating and expansions.

The final milestone by late May to mid-Jun required the use of advanced units and technology upgrades, with possible features such as improved expansion, scout harassment or enemy economy disruption. It was somewhat open-ended, as it was unknown which features would be most needed at the time.

The first two milestones were completed with all required features. By the third milestone however, the expansion feature proved much more time-consuming to implement than predicted. Meanwhile, preliminary results with advanced units suggested a lot of work was required to make them more efficient than just using basic units. For those reasons, advanced units as a feature was scrapped in favor of completing the expansion feature. As with advanced units, expansions were only worthwhile if they were efficient, which required a generalized implementation, better expansion locations and the Maynard slide technique. As a result, the last milestone goals were not satisfied, as improved expansion had been implemented, but the rest was not. On the other hand, in the final version of the agent, all the features had been implemented with good gameplay performance, few and minor bugs and with no reported crashes.

The final features probably took longer than predicted, since they had to be at least as efficient as the rest of the agent's features to be worthwhile. It was easy to implement a simple version of the features, but gameplay performance demanded the consideration of many edge-cases to be efficient. Implementing a single feature well would have a better chance of increasing win-rate.